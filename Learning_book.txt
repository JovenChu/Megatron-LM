一、概念及设置
1. Megatron的并行
    （1）主要的代码是pretrain_gpt.py，而并行的启动、配置设置就是在example/gpt3文件夹中进行声明
    （2）其中启动pretrain_gpt.py核心的是model_provider方法，这里初始化了GPT模型，并引入了上面声明的参数
    （3）pretrain函数：在megaton/training/training.py中，P152行处，初始化进程组
    （4）get_model函数：在330行，设置模型，分布式启动N个进程，每个进程中间有GPT的模型部分层，并通过PP、DP、TP、model、embedding等进程组进行分别处理
    （5）build_train_valid_test_data_iterators函数：在1454行，对数据并行进行数据处理，获取三种数据集，然后使用DistributedDataParallel()方法实现分布式数据并行
    （6）train_step：进行训练的代码。

2. 并行设置：以下所有的代码都在——megatron/core/parallel_state.py
    （1）配置初始化：通过initialize_model_parallel()方法函数（298行）进行模型分组，并初始化进程相关的全局变量，包括tensor、pipeline、model、data、embedding等group组合，定义分组的大小。
    （2）先对模型进行切分（TP+PP）
       （a）TP是横向将张量的计算（矩阵的行、列切分）分成8个组进行处理，组间同名的子模块可以数据并行，每个组是PP流水线的一个stage
       （b）pipeline是竖向将模型的层分给4个组处理，分组的策略是每隔n//p个取一个gpu构成一组，组内的GPU进行串行执行模型
    （3）然后对数据进行切分（DP）data并行部分分为8组，每组2个gpu卡共同对一份数据进行并行处理，一个组在同一个通讯域里面
        
        ```json
        create 8 tensor model-parallel groups, 4 pipeline model-parallel groups
        and 8 data-parallel groups as:
        8 data_parallel groups:
            [g0, g2], [g1, g3], [g4, g6], [g5, g7], [g8, g10], [g9, g11], [g12, g14], [g13, g15]
        8 tensor model-parallel groups:
            [g0, g1], [g2, g3], [g4, g5], [g6, g7], [g8, g9], [g10, g11], [g12, g13], [g14, g15]
        4 pipeline model-parallel groups:
            [g0, g4, g8, g12], [g1, g5, g9, g13], [g2, g6, g10, g14], [g3, g7, g11, g15]
        ```
        
   （4）所有并行配制及处理进程：group进程组就是通信域
        
    （5）TP的分组：在megatron/core/parallel_state.py#L348中的get_tensor_model_parallel_group()函数中进行正向forward的权重切分（521行构建、723行使用），并在15行的reduce函数，进行反向传播的时候对tensor（weight）的all_reduce操作进行汇总。节点内跨机器的通讯，使用组内通讯——tensor_model_parapllel_group即可
   （6）PP进程分配：跨机器跨节点的通信，使用P2POp点对点的通信方式（573行构建、732行使用，通信是在p2p_communication.py的120行中）
        
   （7）DD数据并行进程分配：（在485行构建，740行使用）。分组数d = （总GPU/一个模型需要的资源）=n/（t*p）。并且通常是在megatron/util.py中的average_losses_across_data_parallel_group()函数进行使用


二、张量并行——理论
1. 并行原理：
   （1）列切分：需要将输入X进行copy，然后根据参数A的切分结果分到不同GPU进行与输入的矩阵乘计算，最终通过all_gather进行拼接聚合得到最终结果；
   （2）行切分：需要将输入X进行按列的split切分，和参数A一样切n份，然后一起分到不同的GPU进行矩阵乘计算，最终通过all_reduce的相加聚合得到最终结果。

2. 列并行（ColumnParallelLinear）的实现细节，通常用在transformer的LN层标准化结构、add和标准化结构中
   （1）前向传播：只需要对参数进行列切分，与输入相乘，分为Y1、Y2输入不同的NPU中
   （2）反向传播：这里的Y变成了梯度G，先通过g层进行切分，然后通过一个all_reduce得到每个GPU的聚合后的完整梯度输出

3. 行并行（RowParallelLinear），最重要的，可以用在attention结构、add和标准化结构
   (1)前向传播，和列差不多，但是对输入也要有个列切分。其中的g操作通过reduce_from_tensor_model_parallel_region()函数实现
   (2)反向传播，不同的是f中使用all-gather的操作，通过在megatron中的scatter_to_tensor_model_parallel_region()的函数上实现。



三、张量并行——分布式训练模型结构代码

1. 模型配置
   （1）首先在根目录的pretrain_gpt.py文件中定义好要训练的步骤，需要将写好的训练配置（这个可以参考这个目录：examples/gpt3/train_gpt3_175b_distributed.sh的配置）
   （2）通过文件中的model_provider函数（P38），将配置参数传给了megatron/core/models/gpt/gpt_model.py文件的GPTModel函数
   （3）GPTModel类则继承了LanguageModule的类，构建GPT模型

2. Embedding并行
   （1）GPTModel类实例化调用了LanguageModelEmbedding函数，从而构建transformer中的embedding层
   （2）然后去构建embedding层的两种嵌入：word（词）、position（位置）
   （3）其中通过self.word_embeddings = tensor_parallel.VocabParallelEmbedding去构建张量的并行策略
     （a）megatron/core/tensor_parallel/layers.py中定义了并行函数VocabParallelEmbedding
     （b）需要先定义并行维度tp_size = 2(2张卡)、vocal_size=50000(5万的token字典)、embedding_dim=1024(将每个token映射到1024的向量维度)。所以张量的并行就是NPU0负责0-2.5w的token映射，NPU1负责2.5w-5w的映射
      （c）然后通过一个all_reduce的操作，将两个NPU的输出output_parallel进行聚合，生成完整的input embedding shape[s(sequence len,1024), b(batch size,32), h(hidden size,1024)]

3. layerNorm并行
   （1）作用：分为post（后置）和pre（前置）。归一化的操作，防止输入的大小差异，造成模型参数学习的不均；
   （2）在gpt_model.py文件中的TransformerBlock函数类中定义，这里面使用的是poet_layer_norm
   （3）然后再transformer_block.py文件中，默认使用self.final_layernorm = TENorm()进行各种归一化操作，这里没有开启SP，不用进行并行。其中TENorm是英伟达构建的一个加速库transformer_engine.py中，定义了LayerNorm、RMSNorm等归一化的操作，可以直接调用

4. attention并行（核心核心！！！）
   （1）在pretrain_gpt.py的代码中，函数类model_provider中定义了调用transformer_layer_spec = get_gpt_layer_local_spec()函数来初始化attention层的操作
   （2）然后通过调用SelfAttentionSubmodules函数，来实现self attention的计算过程
   （3）其中最核心的是怎么进行qkv的线性两两计算注意力值，是通过ColumnParallelLinear(列切分)将输入[s,b,h]转化为[s,b,3h]，主要是为了将输出分割为qkv3个矩阵后，按列切分为[s,b,3h/2]执行self attention计算
   （4）这里的并行是将定义的16个head切分到n张卡中进行并行的attention计算。其中计算最核心的代码在megatron/core/transformer/dot_product_attention.py的DotProductAttention类的forward函数，计算完之后还能进行多个head的合并、shape的转化等，最终输出的shape为[s,b,hp]
      （a）重点！！！：每个头上都可以独立计算，最后再将结果concat起来。也就是说，可以把每个头的参数放到一块GPU上。对注意力层A（self-attention），三个参数矩阵Q，K，V，按照“列切割”，每个头放到一块GPU上，做并行计算。对线性层B（Dropout），按照“行切割”。切割的方式和MLP层基本一致，其forward与backward原理也一致
      （b）在实际应用中，并不一定按照一个head占用一块GPU来切割权重，我们也可以一个多个head占用一块GPU，这依然不会改变单块GPU上独立计算的目的。所以实际设计时，我们尽量保证head总数能被GPU个数整除。

   （5）列切分：RowParallelLinear实现，输出shape为[s,b,h]


5. dropout：实际在gpt_layer_specs.py中使用的是get_bias_dropout_add融合算子实现，防止过拟合

6. add操作：使用megatron/core/fusions/fused_bias_dropout.py中的_bias_dropout_add_func执行残差residual连接，作为下一个layer norm的输入，shape为[s,b,h]


7. MLP模块：两层的GELU
（1）megatron/core/models/gpt/gpt_layer_specs.py中定义的_get_mlp_module_spec函数实现，通过MLPSubmodules注册两个线性层A、B，实现B的行并行RowParallelLinear、A的列并行ColumnParallelLinear计算，这样尽量保证各GPU上的计算相互独立，减少通讯量。对A来说，需要做一次GELU激活的计算，而GELU函数是非线形的

def _get_mlp_module_spec(
    use_te: bool = True, num_experts: int = None, moe_grouped_gemm: bool = False
) -> ModuleSpec:
    if num_experts is None:
        # Dense MLP w/ or w/o TE modules.
        return ModuleSpec(
            module=MLP,
            submodules=MLPSubmodules(
                linear_fc1=TELayerNormColumnParallelLinear if use_te else ColumnParallelLinear,
                linear_fc2=TERowParallelLinear if use_te else RowParallelLinear,
            ),
        )

（2）第一个MLP进行权重A的列切分后，输出[s,b,4h]（网络模型结构决定，h to 4h）。然后分两张卡[s,b,2h]进行GeLU的激活处理后，输入到第二个MLP线性层实现权重B的行切分，输出[s,b,h](实现了4h to h)，最终两张卡进行all_reduce的聚合操作。

8. 作用：激活显存占用，能降低多层transformer的显存占用，分到多张卡里完成训练。












